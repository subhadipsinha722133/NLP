{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1>Word2Vec</h1>\n",
        "Word2Vec is a technique used to generate word embeddings, which are vector representations of words that capture semantic relationships between them. These embeddings are learned from a large corpus of text using shallow neural networks, allowing for mathematical operations on words and the ability to infer meaning based on context. <br><br>\n",
        "Here's a more detailed explanation:-<br>\n",
        "Core Idea: Word2Vec aims to represent words as vectors in a multi-dimensional space, where similar words are located closer to each other. This allows for mathematical operations on words, such as \"king - man + woman = queen\", to produce meaningful results. <br>\n",
        "How it Works:-<br>\n",
        "Word Embeddings: Word2Vec learns these vector representations, also known as word embeddings, by analyzing the context in which words appear in a large text corpus. <br>\n",
        "Neural Networks:- <br>It utilizes shallow neural networks (specifically Continuous Bag-of-Words (CBOW) and Skip-gram) to learn these embeddings. <br>\n",
        "CBOW: Predicts a target word based on its surrounding context words. <br>\n",
        "Skip-gram: Predicts the surrounding context words given a target word. <br>\n",
        "Semantic Relationships: Words with similar meanings will have similar vector representations, enabling the capture of semantic relationships. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1>Demo</h1><br>We will use the pre-trained weights of word2vec that was trained on Google New\n",
        "corpus containing 3 billion words. This model consists of 300-dimensional vectors\n",
        "for 3 million words and phrases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gensim\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec,KeyedVectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJIToJ12EsJV",
        "outputId": "2cc688f3-8d11-4793-b9aa-733b80c2779e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py): started\n",
            "  Building wheel for wget (setup.py): finished with status 'done'\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9712 sha256=286c4955e0bf9cbefe996c409941ba849dee887823f4159c7c8a810c7afe2544\n",
            "  Stored in directory: c:\\users\\sinha\\appdata\\local\\pip\\cache\\wheels\\40\\b3\\0f\\a40dbd1c6861731779f62cc4babcb234387e11d697df70ee97\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  DEPRECATION: Building 'wget' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'wget'. Discussion can be found at https://github.com/pypa/pip/issues/6334\n"
          ]
        }
      ],
      "source": [
        "!pip install wget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxjTPj38ExJK",
        "outputId": "d05211ea-d84e-4e0e-bb2e-dfb2920095d8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_YYz4vZCCuV"
      },
      "outputs": [],
      "source": [
        "# Load the pre-trained Word2Vec model\n",
        "# The model is trained on Google News dataset\n",
        "# The model contains 300-dimensional vectors for 3 million words and phrases\n",
        "# The model is in binary format\n",
        "# The model is loaded using KeyedVectors for efficient memory usage\n",
        "\n",
        "model = KeyedVectors.load_word2vec_format(r'D:\\CODING\\PYTHON\\NLP\\GoogleNews-vectors-negative300.bin',binary=True,limit=500000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxh78F7TEeEs",
        "outputId": "8d6c2b53-a330-49e1-9cf8-48056632fd1a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([-3.67187500e-01, -1.21582031e-01,  2.85156250e-01,  8.15429688e-02,\n",
              "        3.19824219e-02, -3.19824219e-02,  1.34765625e-01, -2.73437500e-01,\n",
              "        9.46044922e-03, -1.07421875e-01,  2.48046875e-01, -6.05468750e-01,\n",
              "        5.02929688e-02,  2.98828125e-01,  9.57031250e-02,  1.39648438e-01,\n",
              "       -5.41992188e-02,  2.91015625e-01,  2.85156250e-01,  1.51367188e-01,\n",
              "       -2.89062500e-01, -3.46679688e-02,  1.81884766e-02, -3.92578125e-01,\n",
              "        2.46093750e-01,  2.51953125e-01, -9.86328125e-02,  3.22265625e-01,\n",
              "        4.49218750e-01, -1.36718750e-01, -2.34375000e-01,  4.12597656e-02,\n",
              "       -2.15820312e-01,  1.69921875e-01,  2.56347656e-02,  1.50146484e-02,\n",
              "       -3.75976562e-02,  6.95800781e-03,  4.00390625e-01,  2.09960938e-01,\n",
              "        1.17675781e-01, -4.19921875e-02,  2.34375000e-01,  2.03125000e-01,\n",
              "       -1.86523438e-01, -2.46093750e-01,  3.12500000e-01, -2.59765625e-01,\n",
              "       -1.06933594e-01,  1.04003906e-01, -1.79687500e-01,  5.71289062e-02,\n",
              "       -7.41577148e-03, -5.59082031e-02,  7.61718750e-02, -4.14062500e-01,\n",
              "       -3.65234375e-01, -3.35937500e-01, -1.54296875e-01, -2.39257812e-01,\n",
              "       -3.73046875e-01,  2.27355957e-03, -3.51562500e-01,  8.64257812e-02,\n",
              "        1.26953125e-01,  2.21679688e-01, -9.86328125e-02,  1.08886719e-01,\n",
              "        3.65234375e-01, -5.66406250e-02,  5.66406250e-02, -1.09375000e-01,\n",
              "       -1.66992188e-01, -4.54101562e-02, -2.00195312e-01, -1.22558594e-01,\n",
              "        1.31835938e-01, -1.31835938e-01,  1.03027344e-01, -3.41796875e-01,\n",
              "       -1.57226562e-01,  2.04101562e-01,  4.39453125e-02,  2.44140625e-01,\n",
              "       -3.19824219e-02,  3.20312500e-01, -4.41894531e-02,  1.08398438e-01,\n",
              "       -4.98046875e-02, -9.52148438e-03,  2.46093750e-01, -5.59082031e-02,\n",
              "        4.07714844e-02, -1.78222656e-02, -2.95410156e-02,  1.65039062e-01,\n",
              "        5.03906250e-01, -2.81250000e-01,  9.81445312e-02,  1.80664062e-02,\n",
              "       -1.83593750e-01,  2.53906250e-01,  2.25585938e-01,  1.63574219e-02,\n",
              "        1.81640625e-01,  1.38671875e-01,  3.33984375e-01,  1.39648438e-01,\n",
              "        1.45874023e-02, -2.89306641e-02, -8.39843750e-02,  1.50390625e-01,\n",
              "        1.67968750e-01,  2.28515625e-01,  3.59375000e-01,  1.22558594e-01,\n",
              "       -3.28125000e-01, -1.56250000e-01,  2.77343750e-01,  1.77001953e-02,\n",
              "       -1.46484375e-01, -4.51660156e-03, -4.46777344e-02,  1.75781250e-01,\n",
              "       -3.75000000e-01,  1.16699219e-01, -1.39648438e-01,  2.55859375e-01,\n",
              "       -1.96289062e-01, -2.57568359e-02, -5.41992188e-02, -2.51464844e-02,\n",
              "       -1.93359375e-01, -3.17382812e-02, -8.74023438e-02, -1.32812500e-01,\n",
              "       -2.12402344e-02,  4.33593750e-01, -5.20019531e-02,  3.46679688e-02,\n",
              "        8.00781250e-02,  3.41796875e-02,  1.99218750e-01, -2.39257812e-02,\n",
              "       -2.37304688e-01,  1.93359375e-01,  7.32421875e-02, -2.87109375e-01,\n",
              "        1.25000000e-01,  8.44726562e-02,  1.30859375e-01, -2.19726562e-01,\n",
              "       -1.61132812e-01, -2.63671875e-01, -5.46875000e-01, -2.96875000e-01,\n",
              "        3.44238281e-02, -2.87109375e-01, -1.93359375e-01, -1.61132812e-01,\n",
              "       -3.84765625e-01, -2.14843750e-01, -6.22558594e-03, -1.27929688e-01,\n",
              "       -1.00097656e-01, -6.21093750e-01,  3.78906250e-01, -4.58984375e-01,\n",
              "        1.44531250e-01, -9.13085938e-02, -3.08593750e-01,  2.23632812e-01,\n",
              "        7.86132812e-02, -2.16796875e-01,  8.78906250e-02, -1.66992188e-01,\n",
              "        1.14746094e-02, -2.53906250e-01, -6.25000000e-02,  6.04248047e-03,\n",
              "        1.56250000e-01,  4.37500000e-01, -2.23632812e-01, -2.32421875e-01,\n",
              "        2.75390625e-01,  2.39257812e-01,  4.49218750e-02, -7.51953125e-02,\n",
              "        5.74218750e-01, -2.61230469e-02, -1.21582031e-01,  2.44140625e-01,\n",
              "       -3.37890625e-01,  8.59375000e-02, -7.71484375e-02,  4.85839844e-02,\n",
              "        1.43554688e-01,  4.25781250e-01, -4.29687500e-02, -1.08398438e-01,\n",
              "        1.19628906e-01, -1.91406250e-01, -2.12890625e-01, -2.87109375e-01,\n",
              "       -1.14746094e-01, -2.04101562e-01, -2.06298828e-02, -2.53906250e-01,\n",
              "        8.25195312e-02, -3.97949219e-02, -1.57226562e-01,  1.34765625e-01,\n",
              "        2.08007812e-01, -1.78710938e-01, -2.00195312e-02, -8.34960938e-02,\n",
              "       -1.20605469e-01,  4.29687500e-02, -1.94335938e-01, -1.32812500e-01,\n",
              "       -2.17285156e-02, -2.35351562e-01, -3.63281250e-01,  1.51367188e-01,\n",
              "        9.32617188e-02,  1.63085938e-01,  1.02050781e-01, -4.27734375e-01,\n",
              "        2.83203125e-01,  2.74658203e-04, -3.20312500e-01,  1.68457031e-02,\n",
              "        4.06250000e-01, -5.24902344e-02,  7.91015625e-02, -1.41601562e-01,\n",
              "        5.27343750e-01, -1.26953125e-01,  4.74609375e-01, -6.64062500e-02,\n",
              "        3.41796875e-01, -1.78710938e-01,  3.69140625e-01, -2.05078125e-01,\n",
              "        5.82885742e-03, -1.84570312e-01, -8.88671875e-02, -1.81640625e-01,\n",
              "       -4.80957031e-02,  4.39453125e-01,  2.12890625e-01, -3.07617188e-02,\n",
              "        9.32617188e-02,  2.40234375e-01,  2.39257812e-01,  2.51953125e-01,\n",
              "       -1.98974609e-02,  1.24511719e-01, -4.73632812e-02, -2.13623047e-02,\n",
              "        3.12500000e-02,  3.05175781e-02,  2.79296875e-01,  9.08203125e-02,\n",
              "       -2.02148438e-01, -2.19726562e-02, -2.63671875e-01,  8.78906250e-02,\n",
              "       -1.07421875e-01, -2.49023438e-01, -1.22070312e-02,  1.73828125e-01,\n",
              "       -9.91210938e-02,  7.27539062e-02,  2.59765625e-01, -4.60937500e-01,\n",
              "        3.59375000e-01, -2.25585938e-01,  1.87988281e-02, -2.19726562e-01,\n",
              "       -2.08984375e-01, -1.51367188e-01,  8.64257812e-02,  1.11694336e-02,\n",
              "        6.93359375e-02, -2.99072266e-02,  1.43554688e-01,  1.89453125e-01,\n",
              "       -1.32812500e-01,  4.72656250e-01, -1.40625000e-01, -2.52685547e-02,\n",
              "        1.91406250e-01, -2.63671875e-01, -1.39648438e-01,  1.09375000e-01,\n",
              "        1.97753906e-02,  2.49023438e-01, -1.42578125e-01,  4.15039062e-02],\n",
              "      dtype=float32)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Example usage of the model\n",
        "# Finding similar words to 'cricket'\n",
        "# This will return the top 10 most similar words to 'cricket'\n",
        "# This is useful for understanding the context and relationships of words in the model\n",
        "# You can use this to find synonyms or related terms in the context of sports\n",
        "# You can also use this to explore the semantic space of the model\n",
        "\n",
        "model['cricket']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0vZan3kH1uQ",
        "outputId": "a76e7d71-af9e-487d-a65e-a20f5f5e8213"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('woman', 0.7664012908935547),\n",
              " ('boy', 0.6824871301651001),\n",
              " ('teenager', 0.6586930155754089),\n",
              " ('teenage_girl', 0.6147903203964233),\n",
              " ('girl', 0.5921714305877686),\n",
              " ('robber', 0.5585119128227234),\n",
              " ('Robbery_suspect', 0.5584409832954407),\n",
              " ('teen_ager', 0.5549196600914001),\n",
              " ('men', 0.5489763021469116),\n",
              " ('guy', 0.5420035123825073)]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# most_similar method can be used to find words similar to a given word\n",
        "# This will return the top 10 most similar words\n",
        "# You can use this to find synonyms or related terms\n",
        "\n",
        "\n",
        "model.most_similar('man')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8__ZCNCagH3",
        "outputId": "5c757272-c14e-486a-e0c3-bb3c23b0e270"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('cricketing', 0.8372225761413574),\n",
              " ('cricketers', 0.8165745735168457),\n",
              " ('Test_cricket', 0.8094819188117981),\n",
              " ('Twenty##_cricket', 0.8068488240242004),\n",
              " ('Twenty##', 0.7624265551567078),\n",
              " ('Cricket', 0.75413978099823),\n",
              " ('cricketer', 0.7372578382492065),\n",
              " ('twenty##', 0.7316356897354126),\n",
              " ('T##_cricket', 0.7304614186286926),\n",
              " ('West_Indies_cricket', 0.6987985968589783)]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.most_similar('cricket')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcuq_YPoaziq",
        "outputId": "9968e599-373a-43eb-a58b-d20526f880a2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Facebook', 0.7563533186912537),\n",
              " ('FaceBook', 0.7076998949050903),\n",
              " ('twitter', 0.6988552212715149),\n",
              " ('myspace', 0.6941817998886108),\n",
              " ('Twitter', 0.664244532585144),\n",
              " ('Facebook.com', 0.6529868245124817),\n",
              " ('FacebookFacebook', 0.6162722110748291),\n",
              " ('facebook.com', 0.6135972142219543),\n",
              " ('Twitter.com', 0.6102108359336853),\n",
              " ('TwitterTwitter', 0.6085205674171448)]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.most_similar('facebook') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlUXi8KKa33_",
        "outputId": "f0ed1c57-e0b0-4b63-aa10-c081cf56e6f1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.76640123"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# You can also find the similarity between two words\n",
        "# This will return a float value between -1 and 1\n",
        "# A value closer to 1 means the words are similar, while a value closer to -1 means they are dissimilar\n",
        "# This is useful for understanding the semantic relationship between words\n",
        "\n",
        "\n",
        "model.similarity('man','woman')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZk8zVzbbE9L",
        "outputId": "3ca7ff42-f489-45a7-b70f-5a8233c1e424"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-0.032995153"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.similarity('man','PHP')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "uApAHCO5db2G",
        "outputId": "e07863b5-ab32-4ea6-f395-42dc87b1da27"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'monkey'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# You can also find words that do not match a given set of words\n",
        "# This will return the word that is least similar to the others\n",
        "# This is useful for finding outliers or unrelated terms in a set of words\n",
        "# For example, if you have a set of programming languages and one word that is not a programming language, this method will return the non-programming language word\n",
        "# This can help in identifying terms that do not fit the context of the others\n",
        "# This is useful for tasks like word analogy or finding unrelated terms in a set of words\n",
        "\n",
        "\n",
        "\n",
        "model.doesnt_match(['PHP','java','monkey'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKZ9ERK1bcGY",
        "outputId": "78103810-5a07-4299-8b1f-8546c43dd1fb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('king', 0.8449392318725586),\n",
              " ('queen', 0.7300517559051514),\n",
              " ('monarch', 0.645466148853302),\n",
              " ('princess', 0.6156251430511475),\n",
              " ('crown_prince', 0.5818676352500916),\n",
              " ('prince', 0.5777117609977722),\n",
              " ('kings', 0.5613663792610168),\n",
              " ('sultan', 0.5376775860786438),\n",
              " ('queens', 0.5289887189865112),\n",
              " ('ruler', 0.5247419476509094)]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# You can also perform vector arithmetic with the model\n",
        "# For example, to find a word that is similar to 'king\n",
        "\n",
        "\n",
        "vec = model['king'] - model['man'] + model['woman']\n",
        "model.most_similar([vec])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3i6mgOrcCln",
        "outputId": "8054eefa-b037-4588-eeb3-9a8277cce896"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('INR', 0.6442341208457947),\n",
              " ('GBP', 0.5040826797485352),\n",
              " ('England', 0.44649264216423035),\n",
              " ('£', 0.43340998888015747),\n",
              " ('Â_£', 0.4307197630405426),\n",
              " ('£_#.##m', 0.42561301589012146),\n",
              " ('Pounds_Sterling', 0.42512619495391846),\n",
              " ('GBP##', 0.42464491724967957),\n",
              " ('stg', 0.42324796319007874),\n",
              " ('£_#.###m', 0.4201711118221283)]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# You can also perform vector arithmetic with other words\n",
        "# For example, to find a word that is similar to 'India' in the context of 'England'\n",
        "# This will return the top 10 most similar words to the resulting vector\n",
        "# This is useful for exploring relationships between countries or other entities in the model\n",
        "\n",
        "\n",
        "vec = model['INR'] - model ['India'] + model['England']\n",
        "model.most_similar([vec])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qc9CCPdLcgpP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ENV1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
